{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.llms import Ollama\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform catalog to be used in a RAG context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_sql_dump(sql_dump: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes SQL-style comments, flattens newlines, and condenses extra spaces.\n",
    "    \"\"\"\n",
    "    # Remove line-based SQL comments (e.g. -- comment)\n",
    "    sql_dump = re.sub(r'--.*?$', '', sql_dump, flags=re.MULTILINE)\n",
    "    # Remove /* */ block comments if present\n",
    "    sql_dump = re.sub(r'/\\*.*?\\*/', '', sql_dump, flags=re.DOTALL)\n",
    "    # Replace newlines with spaces\n",
    "    sql_dump = re.sub(r'\\n', ' ', sql_dump)\n",
    "    # Replace multiple spaces with single space\n",
    "    sql_dump = re.sub(r'\\s{2,}', ' ', sql_dump)\n",
    "    # Trim leading/trailing whitespace\n",
    "    sql_dump = sql_dump.strip()\n",
    "    return sql_dump\n",
    "\n",
    "\n",
    "def extract_schema_info(sql_dump: str):\n",
    "    \"\"\"\n",
    "    Extracts table definitions, primary keys, foreign keys, and\n",
    "    a simple 'joins' mapping from a cleaned SQL dump.\n",
    "    \"\"\"\n",
    "    # Regex patterns\n",
    "    # This pattern handles optional schema name (e.g. CREATE TABLE public.actor ...)\n",
    "    # and captures everything inside the parentheses up to the matching semicolon.\n",
    "    table_pattern = re.compile(\n",
    "        r'CREATE TABLE\\s+'\n",
    "        r'(?:[\"]?(\\w+)[\"]?\\.)?'   # optional schema name (group 1)\n",
    "        r'[\"]?(\\w+)[\"]?'\n",
    "        r'\\s*\\((.*?)\\)\\s*;', \n",
    "        re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "\n",
    "    pkey_pattern = re.compile(\n",
    "        r'ALTER TABLE ONLY\\s+'\n",
    "        r'(?:[\"]?(\\w+)[\"]?\\.)?'   # optional schema name\n",
    "        r'[\"]?(\\w+)[\"]?'\n",
    "        r'\\s+ADD CONSTRAINT\\s+(\\w+)\\s+PRIMARY KEY\\s*\\((.*?)\\);',\n",
    "        re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "\n",
    "    fkey_pattern = re.compile(\n",
    "        r'ALTER TABLE ONLY\\s+'\n",
    "        r'(?:[\"]?(\\w+)[\"]?\\.)?'   # optional schema name\n",
    "        r'[\"]?(\\w+)[\"]?'\n",
    "        r'\\s+ADD CONSTRAINT\\s+(\\w+)\\s+FOREIGN KEY\\s*\\((.*?)\\)\\s+REFERENCES\\s+'\n",
    "        r'(?:[\"]?(\\w+)[\"]?\\.)?'   # optional ref schema\n",
    "        r'[\"]?(\\w+)[\"]?\\s*\\((.*?)\\)'\n",
    "        r'(?:\\s+ON UPDATE \\w+\\s+ON DELETE \\w+)?;', \n",
    "        re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "\n",
    "    # Data structures to fill\n",
    "    tables = {}\n",
    "    primary_keys = {}\n",
    "    foreign_keys = {}\n",
    "    joins = {}\n",
    "\n",
    "    # Extract tables\n",
    "    for match in table_pattern.finditer(sql_dump):\n",
    "        schema_name = match.group(1)  # Might be None if no schema specified\n",
    "        table_name = match.group(2)\n",
    "        columns_str = match.group(3)\n",
    "        \n",
    "        # Key for referencing the table will just be table_name\n",
    "        # but you could store \"schema.table\" if needed:\n",
    "        full_table_name = table_name if not schema_name else f\"{schema_name}.{table_name}\"\n",
    "        \n",
    "        tables[full_table_name] = columns_str\n",
    "\n",
    "    # Extract primary keys\n",
    "    for match in pkey_pattern.finditer(sql_dump):\n",
    "        schema_name = match.group(1)\n",
    "        table_name = match.group(2)\n",
    "        pkey_name = match.group(3)\n",
    "        pkey_columns = match.group(4).strip()\n",
    "\n",
    "        full_table_name = table_name if not schema_name else f\"{schema_name}.{table_name}\"\n",
    "        primary_keys[full_table_name] = {\n",
    "            'pkey_name': pkey_name,\n",
    "            'pkey_columns': pkey_columns\n",
    "        }\n",
    "\n",
    "    # Extract foreign keys\n",
    "    for match in fkey_pattern.finditer(sql_dump):\n",
    "        schema_name = match.group(1)\n",
    "        table_name = match.group(2)\n",
    "        fkey_name = match.group(3)\n",
    "        fkey_columns = match.group(4).strip()\n",
    "        ref_schema = match.group(5)\n",
    "        ref_table_name = match.group(6)\n",
    "        ref_columns = match.group(7).strip()\n",
    "\n",
    "        full_table_name = table_name if not schema_name else f\"{schema_name}.{table_name}\"\n",
    "        full_ref_table_name = ref_table_name if not ref_schema else f\"{ref_schema}.{ref_table_name}\"\n",
    "\n",
    "        if full_table_name not in foreign_keys:\n",
    "            foreign_keys[full_table_name] = []\n",
    "\n",
    "        foreign_keys[full_table_name].append({\n",
    "            'fkey_name': fkey_name,\n",
    "            'fkey_columns': fkey_columns,\n",
    "            'ref_table': full_ref_table_name,\n",
    "            'ref_columns': ref_columns\n",
    "        })\n",
    "\n",
    "    # For simplicity, let's say \"joins\" just replicate foreign_keys\n",
    "    joins = foreign_keys\n",
    "\n",
    "    return tables, primary_keys, foreign_keys, joins\n",
    "\n",
    "\n",
    "def create_rag_documents(tables, primary_keys, foreign_keys, joins):\n",
    "    \"\"\"\n",
    "    Converts the extracted schema info into a list of JSON-like documents\n",
    "    suitable for RAG ingestion. Each document contains:\n",
    "      - table_name\n",
    "      - columns (list of columns)\n",
    "      - primary_key\n",
    "      - foreign_keys\n",
    "      - joins\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "\n",
    "    for table_name, columns_str in tables.items():\n",
    "        # Split columns by commas that are not inside parentheses\n",
    "        # (to avoid splitting on something like numeric(4,2)).\n",
    "        # A simpler approach is to split by lines or semicolons, but let's do a naive approach:\n",
    "        raw_cols = split_columns(columns_str)\n",
    "\n",
    "        # Clean each column definition\n",
    "        cleaned_cols = [clean_column_definition(c) for c in raw_cols if c.strip()]\n",
    "\n",
    "        doc = {\n",
    "            'table_name': table_name,\n",
    "            'columns': cleaned_cols,\n",
    "            'primary_key': primary_keys.get(table_name, {}),\n",
    "            'foreign_keys': foreign_keys.get(table_name, []),\n",
    "            'joins': joins.get(table_name, [])\n",
    "        }\n",
    "        documents.append(doc)\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_columns(columns_block: str):\n",
    "    \"\"\"\n",
    "    Split a CREATE TABLE column block into individual column/constraint lines,\n",
    "    ignoring commas found inside parentheses (like numeric(4,2)).\n",
    "    \"\"\"\n",
    "    # A very common approach is to do a manual parse counting parentheses.\n",
    "    # For brevity, here’s a quick version:\n",
    "    results = []\n",
    "    current = []\n",
    "    paren_depth = 0\n",
    "\n",
    "    for char in columns_block:\n",
    "        if char == '(':\n",
    "            paren_depth += 1\n",
    "            current.append(char)\n",
    "        elif char == ')':\n",
    "            paren_depth -= 1\n",
    "            current.append(char)\n",
    "        elif char == ',' and paren_depth == 0:\n",
    "            # We reached a top-level comma -> new column\n",
    "            results.append(\"\".join(current).strip())\n",
    "            current = []\n",
    "        else:\n",
    "            current.append(char)\n",
    "\n",
    "    # Add the last accumulated column\n",
    "    if current:\n",
    "        results.append(\"\".join(current).strip())\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def clean_column_definition(col_def: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans up one column/constraint definition line by removing extra\n",
    "    semicolons, repeating spaces, etc.\n",
    "    \"\"\"\n",
    "    # Remove trailing semicolons if any\n",
    "    col_def = col_def.rstrip(';')\n",
    "    # Convert multiple spaces to single\n",
    "    col_def = re.sub(r'\\s{2,}', ' ', col_def)\n",
    "    # Trim\n",
    "    col_def = col_def.strip()\n",
    "    return col_def\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dump_path = \"./context/pagila-schema.sql\"\n",
    "with open(sql_dump_path, 'r') as file:\n",
    "    sql_dump = file.read()\n",
    "sql_dump = clean_sql_dump(sql_dump)\n",
    "\n",
    "tables, primary_keys, foreign_keys, joins = extract_schema_info(sql_dump)\n",
    "rag_docs = create_rag_documents(tables, primary_keys, foreign_keys, joins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Ollama\n",
    "from langchain.docstore.document import Document\n",
    "# Convert each dict to a text string or store as JSON\n",
    "documents = []\n",
    "for doc in rag_docs:\n",
    "    # Convert dict to a JSON string or any text representation you prefer\n",
    "    text_content = json.dumps(doc, ensure_ascii=False, indent=2)\n",
    "    documents.append(Document(page_content=text_content))\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2) Create embeddings with a Hugging Face model + FAISS vector store\n",
    "# ------------------------------------------------------\n",
    "# Example: Using the MiniLM model from SentenceTransformers\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(documents, embedding=embedding_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"Here is the data context :\\n\"\n",
    "        \"{context}\\n\\n\"\n",
    "        \"Given this context, generate a sql query answering the following question:\\n\"\n",
    "        \"{question}\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SQL Query: To answer this question, we need to join the `film` table with the `film_category` table.\n",
      "\n",
      "From the context, we can see that the `film_category` table has a foreign key `film_id` that references the `film_id` in the `film` table. This means that for each row in the `film_category` table, there is a corresponding film in the `film` table.\n",
      "\n",
      "To get the list of films with their categories, we can perform an inner join between the two tables on the `film_id` column. The resulting query would be:\n",
      "\n",
      "```\n",
      "SELECT f.title, fc.category_name\n",
      "FROM film f\n",
      "INNER JOIN film_category fc ON f.film_id = fc.film_id;\n",
      "```\n",
      "\n",
      "However, this assumes that there is a `category_name` column in the `category` table, which does not exist in the provided context. To get the category name, we would need to join the `film_category` table with the `category` table on the `category_id` column.\n",
      "\n",
      "Here's an updated query:\n",
      "\n",
      "```\n",
      "SELECT f.title, c.name AS category_name\n",
      "FROM film f\n",
      "INNER JOIN film_category fc ON f.film_id = fc.film_id\n",
      "INNER JOIN category c ON fc.category_id = c.category_id;\n",
      "```\n",
      "\n",
      "This query would return a list of films with their corresponding categories.\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=Ollama(model=\"llama3\"),  # Replace with your LLM\n",
    "    retriever=retriever,\n",
    "    # Optionally, pass a custom prompt if you want to strictly control formatting\n",
    "    # chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 5) Test with a sample question\n",
    "# ------------------------------------------------------\n",
    "question = \"Quelle est la liste des films disponibles avec leur catégorie ?\"\n",
    "response = qa_chain.run(question)\n",
    "\n",
    "print(\"Generated SQL Query:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
