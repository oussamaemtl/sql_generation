{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.llms import Ollama\n",
    "from protected import pwd_db\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform catalog to be used in a RAG context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_sql_dump(sql_dump: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes SQL-style comments, flattens newlines, and condenses extra spaces.\n",
    "    \"\"\"\n",
    "    # Remove line-based SQL comments (e.g. -- comment)\n",
    "    sql_dump = re.sub(r'--.*?$', '', sql_dump, flags=re.MULTILINE)\n",
    "    # Remove /* */ block comments if present\n",
    "    sql_dump = re.sub(r'/\\*.*?\\*/', '', sql_dump, flags=re.DOTALL)\n",
    "    # Replace newlines with spaces\n",
    "    sql_dump = re.sub(r'\\n', ' ', sql_dump)\n",
    "    # Replace multiple spaces with single space\n",
    "    sql_dump = re.sub(r'\\s{2,}', ' ', sql_dump)\n",
    "    # Trim leading/trailing whitespace\n",
    "    sql_dump = sql_dump.strip()\n",
    "    return sql_dump\n",
    "\n",
    "\n",
    "def extract_schema_info(sql_dump: str):\n",
    "    \"\"\"\n",
    "    Extracts table definitions, primary keys, foreign keys, and\n",
    "    a simple 'joins' mapping from a cleaned SQL dump.\n",
    "    \"\"\"\n",
    "    # Regex patterns\n",
    "    # This pattern handles optional schema name (e.g. CREATE TABLE public.actor ...)\n",
    "    # and captures everything inside the parentheses up to the matching semicolon.\n",
    "    table_pattern = re.compile(\n",
    "        r'CREATE TABLE\\s+'\n",
    "        r'(?:[\"]?(\\w+)[\"]?\\.)?'   # optional schema name (group 1)\n",
    "        r'[\"]?(\\w+)[\"]?'\n",
    "        r'\\s*\\((.*?)\\)\\s*;', \n",
    "        re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "\n",
    "    pkey_pattern = re.compile(\n",
    "        r'ALTER TABLE ONLY\\s+'\n",
    "        r'(?:[\"]?(\\w+)[\"]?\\.)?'   # optional schema name\n",
    "        r'[\"]?(\\w+)[\"]?'\n",
    "        r'\\s+ADD CONSTRAINT\\s+(\\w+)\\s+PRIMARY KEY\\s*\\((.*?)\\);',\n",
    "        re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "\n",
    "    fkey_pattern = re.compile(\n",
    "        r'ALTER TABLE ONLY\\s+'\n",
    "        r'(?:[\"]?(\\w+)[\"]?\\.)?'   # optional schema name\n",
    "        r'[\"]?(\\w+)[\"]?'\n",
    "        r'\\s+ADD CONSTRAINT\\s+(\\w+)\\s+FOREIGN KEY\\s*\\((.*?)\\)\\s+REFERENCES\\s+'\n",
    "        r'(?:[\"]?(\\w+)[\"]?\\.)?'   # optional ref schema\n",
    "        r'[\"]?(\\w+)[\"]?\\s*\\((.*?)\\)'\n",
    "        r'(?:\\s+ON UPDATE \\w+\\s+ON DELETE \\w+)?;', \n",
    "        re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "\n",
    "    # Data structures to fill\n",
    "    tables = {}\n",
    "    primary_keys = {}\n",
    "    foreign_keys = {}\n",
    "    joins = {}\n",
    "\n",
    "    # Extract tables\n",
    "    for match in table_pattern.finditer(sql_dump):\n",
    "        schema_name = match.group(1)  # Might be None if no schema specified\n",
    "        table_name = match.group(2)\n",
    "        columns_str = match.group(3)\n",
    "        \n",
    "        # Key for referencing the table will just be table_name\n",
    "        # but you could store \"schema.table\" if needed:\n",
    "        full_table_name = table_name if not schema_name else f\"{schema_name}.{table_name}\"\n",
    "        \n",
    "        tables[full_table_name] = columns_str\n",
    "\n",
    "    # Extract primary keys\n",
    "    for match in pkey_pattern.finditer(sql_dump):\n",
    "        schema_name = match.group(1)\n",
    "        table_name = match.group(2)\n",
    "        pkey_name = match.group(3)\n",
    "        pkey_columns = match.group(4).strip()\n",
    "\n",
    "        full_table_name = table_name if not schema_name else f\"{schema_name}.{table_name}\"\n",
    "        primary_keys[full_table_name] = {\n",
    "            'pkey_name': pkey_name,\n",
    "            'pkey_columns': pkey_columns\n",
    "        }\n",
    "\n",
    "    # Extract foreign keys\n",
    "    for match in fkey_pattern.finditer(sql_dump):\n",
    "        schema_name = match.group(1)\n",
    "        table_name = match.group(2)\n",
    "        fkey_name = match.group(3)\n",
    "        fkey_columns = match.group(4).strip()\n",
    "        ref_schema = match.group(5)\n",
    "        ref_table_name = match.group(6)\n",
    "        ref_columns = match.group(7).strip()\n",
    "\n",
    "        full_table_name = table_name if not schema_name else f\"{schema_name}.{table_name}\"\n",
    "        full_ref_table_name = ref_table_name if not ref_schema else f\"{ref_schema}.{ref_table_name}\"\n",
    "\n",
    "        if full_table_name not in foreign_keys:\n",
    "            foreign_keys[full_table_name] = []\n",
    "\n",
    "        foreign_keys[full_table_name].append({\n",
    "            'fkey_name': fkey_name,\n",
    "            'fkey_columns': fkey_columns,\n",
    "            'ref_table': full_ref_table_name,\n",
    "            'ref_columns': ref_columns\n",
    "        })\n",
    "\n",
    "    # For simplicity, let's say \"joins\" just replicate foreign_keys\n",
    "    joins = foreign_keys\n",
    "\n",
    "    return tables, primary_keys, foreign_keys, joins\n",
    "\n",
    "\n",
    "def create_rag_documents(tables, primary_keys, foreign_keys, joins):\n",
    "    \"\"\"\n",
    "    Converts the extracted schema info into a list of JSON-like documents\n",
    "    suitable for RAG ingestion. Each document contains:\n",
    "      - table_name\n",
    "      - columns (list of columns)\n",
    "      - primary_key\n",
    "      - foreign_keys\n",
    "      - joins\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "\n",
    "    for table_name, columns_str in tables.items():\n",
    "        # Split columns by commas that are not inside parentheses\n",
    "        # (to avoid splitting on something like numeric(4,2)).\n",
    "        # A simpler approach is to split by lines or semicolons, but let's do a naive approach:\n",
    "        raw_cols = split_columns(columns_str)\n",
    "\n",
    "        # Clean each column definition\n",
    "        cleaned_cols = [clean_column_definition(c) for c in raw_cols if c.strip()]\n",
    "\n",
    "        doc = {\n",
    "            'table_name': table_name,\n",
    "            'columns': cleaned_cols,\n",
    "            'primary_key': primary_keys.get(table_name, {}),\n",
    "            'foreign_keys': foreign_keys.get(table_name, []),\n",
    "            'joins': joins.get(table_name, [])\n",
    "        }\n",
    "        documents.append(doc)\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_columns(columns_block: str):\n",
    "    \"\"\"\n",
    "    Split a CREATE TABLE column block into individual column/constraint lines,\n",
    "    ignoring commas found inside parentheses (like numeric(4,2)).\n",
    "    \"\"\"\n",
    "    # A very common approach is to do a manual parse counting parentheses.\n",
    "    # For brevity, hereâ€™s a quick version:\n",
    "    results = []\n",
    "    current = []\n",
    "    paren_depth = 0\n",
    "\n",
    "    for char in columns_block:\n",
    "        if char == '(':\n",
    "            paren_depth += 1\n",
    "            current.append(char)\n",
    "        elif char == ')':\n",
    "            paren_depth -= 1\n",
    "            current.append(char)\n",
    "        elif char == ',' and paren_depth == 0:\n",
    "            # We reached a top-level comma -> new column\n",
    "            results.append(\"\".join(current).strip())\n",
    "            current = []\n",
    "        else:\n",
    "            current.append(char)\n",
    "\n",
    "    # Add the last accumulated column\n",
    "    if current:\n",
    "        results.append(\"\".join(current).strip())\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def clean_column_definition(col_def: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans up one column/constraint definition line by removing extra\n",
    "    semicolons, repeating spaces, etc.\n",
    "    \"\"\"\n",
    "    # Remove trailing semicolons if any\n",
    "    col_def = col_def.rstrip(';')\n",
    "    # Convert multiple spaces to single\n",
    "    col_def = re.sub(r'\\s{2,}', ' ', col_def)\n",
    "    # Trim\n",
    "    col_def = col_def.strip()\n",
    "    return col_def\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dump_path = \"./context/pagila-schema.sql\"\n",
    "with open(sql_dump_path, 'r') as file:\n",
    "    sql_dump = file.read()\n",
    "sql_dump = clean_sql_dump(sql_dump)\n",
    "\n",
    "tables, primary_keys, foreign_keys, joins = extract_schema_info(sql_dump)\n",
    "rag_docs = create_rag_documents(tables, primary_keys, foreign_keys, joins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oelmtili\\AppData\\Local\\Temp\\ipykernel_30500\\3011028475.py:19: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\oelmtili\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Ollama\n",
    "from langchain.docstore.document import Document\n",
    "# Convert each dict to a text string or store as JSON\n",
    "documents = []\n",
    "for doc in rag_docs:\n",
    "    # Convert dict to a JSON string or any text representation you prefer\n",
    "    text_content = json.dumps(doc, ensure_ascii=False, indent=2)\n",
    "    documents.append(Document(page_content=text_content))\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2) Create embeddings with a Hugging Face model + FAISS vector store\n",
    "# ------------------------------------------------------\n",
    "# Example: Using the MiniLM model from SentenceTransformers\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(documents, embedding=embedding_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"Here is the data context :\\n\"\n",
    "        \"{context}\\n\\n\"\n",
    "        \"Given this context, generate a sql query answering the following question:\\n\"\n",
    "        \"{question}\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oelmtili\\AppData\\Local\\Temp\\ipykernel_30500\\256622824.py:4: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm=Ollama(model=\"llama3\"),  # Replace with your LLM\n",
      "C:\\Users\\oelmtili\\AppData\\Local\\Temp\\ipykernel_30500\\256622824.py:14: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SQL Query: A database question!\n",
      "\n",
      "To answer this, we need to analyze the relationships between tables. We can see that there are two relationships:\n",
      "\n",
      "1. `film` has a foreign key `language_id` referencing the `language` table.\n",
      "2. `film_category` has foreign keys `film_id` and `category_id` referencing the `film` and `category` tables, respectively.\n",
      "\n",
      "This suggests that we need to join the `film` table with the `film_category` table on the `film_id` column, and then join the result with the `category` table on the `category_id` column.\n",
      "\n",
      "The final query would be:\n",
      "```sql\n",
      "SELECT f.title, fc.category_name\n",
      "FROM film f\n",
      "JOIN film_category fc ON f.film_id = fc.film_id\n",
      "JOIN category c ON fc.category_id = c.category_id;\n",
      "```\n",
      "This should give us a list of films with their corresponding categories.\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=Ollama(model=\"llama3\"),  # Replace with your LLM\n",
    "    retriever=retriever,\n",
    "    # Optionally, pass a custom prompt if you want to strictly control formatting\n",
    "    # chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 5) Test with a sample question\n",
    "# ------------------------------------------------------\n",
    "question = \"Quelle est la liste des films disponibles avec leur catÃ©gorie ?\"\n",
    "response = qa_chain.run(question)\n",
    "\n",
    "print(\"Generated SQL Query:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding logging context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def get_logging_context(\n",
    "):\n",
    "    \"\"\"\n",
    "    Connect to PostgreSQL and retrieve table structures and some rows\n",
    "    from queries_log and generated_views for an LLM 'logging context'.\n",
    "    \"\"\"\n",
    "    connection_params = {\n",
    "    'dbname': 'pagila',\n",
    "    'user': 'postgres',\n",
    "    'password': pwd_db,\n",
    "    'host': 'localhost',\n",
    "    'port': 5432\n",
    "    }\n",
    "    # Connect to the database\n",
    "    conn = psycopg2.connect(**connection_params)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Helper: fetch columns from information_schema\n",
    "    def fetch_table_structure(table_name):\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT column_name, data_type, is_nullable\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_name = %s\n",
    "            ORDER BY ordinal_position\n",
    "        \"\"\", (table_name,))\n",
    "        columns = cursor.fetchall()\n",
    "        # Build a small text block describing each column\n",
    "        lines = []\n",
    "        for col_name, data_type, is_nullable in columns:\n",
    "            lines.append(f\"  - {col_name} {data_type} {'(nullable)' if is_nullable == 'YES' else '(not null)'}\")\n",
    "        structure_str = \"Columns:\\n\" + \"\\n\".join(lines)\n",
    "        return structure_str\n",
    "\n",
    "    # Helper: fetch sample rows from each table\n",
    "    def fetch_sample_rows(table_name):\n",
    "        cursor.execute(f\"SELECT * FROM {table_name} ORDER BY created_at\")\n",
    "        rows = cursor.fetchall()\n",
    "        # If you want column names, re-fetch from cursor.description:\n",
    "        col_names = [desc.name for desc in cursor.description]\n",
    "\n",
    "        lines = []\n",
    "        for row in rows:\n",
    "            # Zip column names with the row values\n",
    "            row_data = \", \".join(f\"{col}: {val}\" for col, val in zip(col_names, row))\n",
    "            lines.append(\"    \" + row_data)\n",
    "        if not lines:\n",
    "            lines = [\"    No rows found.\"]\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    # Build the logging context\n",
    "    logging_context_lines = [\"-- LOGGING SCHEMA METADATA --\"]\n",
    "\n",
    "    for table_name in [\"queries_log\", \"generated_views\"]:\n",
    "        logging_context_lines.append(f\"Table: {table_name}\")\n",
    "        # structure\n",
    "        table_structure = fetch_table_structure(table_name)\n",
    "        logging_context_lines.append(table_structure)\n",
    "        # sample rows\n",
    "        sample_rows = fetch_sample_rows(table_name)\n",
    "        logging_context_lines.append(\"Sample rows (up to 5):\\n\" + sample_rows)\n",
    "        logging_context_lines.append(\"\")  # blank line\n",
    "\n",
    "    # Combine into a single text block\n",
    "    logging_context = \"\\n\".join(logging_context_lines)\n",
    "\n",
    "    # Clean up\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    return logging_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- LOGGING SCHEMA METADATA --\n",
      "Table: queries_log\n",
      "Columns:\n",
      "  - query_id integer (not null)\n",
      "  - question text (not null)\n",
      "  - description text (nullable)\n",
      "  - sql_text text (not null)\n",
      "  - created_at timestamp without time zone (not null)\n",
      "Sample rows (up to 5):\n",
      "    No rows found.\n",
      "\n",
      "Table: generated_views\n",
      "Columns:\n",
      "  - view_id integer (not null)\n",
      "  - query_id integer (not null)\n",
      "  - view_name text (not null)\n",
      "  - created_at timestamp without time zone (not null)\n",
      "Sample rows (up to 5):\n",
      "    No rows found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_context = get_logging_context()\n",
    "print(log_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schema context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "sql_dump_path = \"./context/pagila-schema.sql\"\n",
    "with open(sql_dump_path, 'r') as file:\n",
    "    sql_dump = file.read()\n",
    "sql_dump = clean_sql_dump(sql_dump)\n",
    "\n",
    "tables, primary_keys, foreign_keys, joins = extract_schema_info(sql_dump)\n",
    "rag_docs = create_rag_documents(tables, primary_keys, foreign_keys, joins)\n",
    "\n",
    "documents = []\n",
    "for doc in rag_docs:\n",
    "    text_content = json.dumps(doc, ensure_ascii=False, indent=2)\n",
    "    documents.append(Document(page_content=text_content))\n",
    "\n",
    "# Create embeddings + FAISS vector store\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(documents, embedding=embedding_model)\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "multi_context_prompt = PromptTemplate(\n",
    "    input_variables=[\"schema_context\", \"logging_context\", \"question\"],\n",
    "    template=(\n",
    "        \"You have two separate pieces of context:\\n\\n\"\n",
    "        \"=== SCHEMA CONTEXT (Main DB) ===\\n\"\n",
    "        \"{schema_context}\\n\\n\"\n",
    "        \"=== LOGGING CONTEXT (queries_log & generated_views) ===\\n\"\n",
    "        \"{logging_context}\\n\\n\"\n",
    "        \"The user wants two main outputs:\\n\\n\"\n",
    "        \"1) A valid SQL query (SELECT, JOIN, etc.) that answers the question.\\n\"\n",
    "        \"2) One or more SQL statements to log this query:\\n\"\n",
    "        \"   - An INSERT INTO queries_log, with:\\n\"\n",
    "        \"       * question: the exact user question\\n\"\n",
    "        \"       * description: a short description of what the query does (NOT NULL)\\n\"\n",
    "        \"       * sql_text: the exact SQL query from (1)\\n\"\n",
    "        \"       * created_at: use NOW() or CURRENT_TIMESTAMP\\n\"\n",
    "        \"     (Do not set them to NULL. Provide real values.)\\n\\n\"\n",
    "        \"   - a CREATE VIEW statement.\\n\"\n",
    "        \"   - an INSERT INTO generated_views referencing that new view.\\n\\n\"\n",
    "        \"Important:\\n\"\n",
    "        \"- Do NOT output NULL for 'description' or 'sql_text'.\\n\"\n",
    "        \"- Use the exact same SQL text for 'sql_text' that you generated in step (1).\\n\"\n",
    "        \"- If query_id and view_id are SERIAL or auto-increment, use 'DEFAULT' or omit the column, or use RETURNING.\\n\"\n",
    "        \"- Please produce these statements in **four** separate code blocks, labeled:\\n\"\n",
    "        \"  1) -- Final Query\\n\"\n",
    "        \"  2) -- Insert into queries_log\\n\"\n",
    "        \"  3) -- Create or replace view\\n\"\n",
    "        \"  4) -- Insert into generated_views\\n\\n\"\n",
    "        \"The question is:\\n\"\n",
    "        \"\\\"{question}\\\"\\n\\n\"\n",
    "        \"Now provide the standardized output exactly as requested.\\n\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oelmtili\\AppData\\Local\\Temp\\ipykernel_30500\\1247580640.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(question)\n",
      "C:\\Users\\oelmtili\\AppData\\Local\\Temp\\ipykernel_30500\\1247580640.py:24: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(final_prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      " Here are the outputs:\n",
      "\n",
      "**1) Final Query**\n",
      "SELECT m.title, c.name\n",
      "FROM film_category fc\n",
      "JOIN category c ON fc.category_id = c.category_id\n",
      "JOIN film m ON fc.film_id = m.film_id;\n",
      "\n",
      "**2) Insert into queries_log**\n",
      "INSERT INTO queries_log (question, description, sql_text, created_at)\n",
      "VALUES ('Quelle est la liste des films disponibles avec leur catÃ©gorie ?', 'List of available movies with their categories', \n",
      "         'SELECT m.title, c.name\n",
      "          FROM film_category fc\n",
      "          JOIN category c ON fc.category_id = c.category_id\n",
      "          JOIN film m ON fc.film_id = m.film_id;', NOW());\n",
      "\n",
      "**3) Create or replace view**\n",
      "CREATE OR REPLACE VIEW available_movies AS\n",
      "SELECT m.title, c.name\n",
      "FROM film_category fc\n",
      "JOIN category c ON fc.category_id = c.category_id\n",
      "JOIN film m ON fc.film_id = m.film_id;\n",
      "\n",
      "**4) Insert into generated_views**\n",
      "INSERT INTO generated_views (view_name, query_id)\n",
      "VALUES ('available_movies', DEFAULT);\n",
      "\n",
      "Please note that I've assumed the `query_id` is a serial column in both `queries_log` and `generated_views`. If it's not, you can modify the `DEFAULT` value accordingly.\n"
     ]
    }
   ],
   "source": [
    "# 1) Retrieve from vectorstore\n",
    "question = \"Quelle est la liste des films disponibles avec leur catÃ©gorie ?\"\n",
    "retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "# Combine them into a single string for the 'schema_context'\n",
    "schema_context_str = \"\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "# 2) Retrieve logging context from Postgres\n",
    "logging_context_str = get_logging_context()\n",
    "\n",
    "# 3) Format the final prompt\n",
    "final_prompt = multi_context_prompt.format(\n",
    "    schema_context=schema_context_str,\n",
    "    logging_context=logging_context_str,\n",
    "    question=question\n",
    ")\n",
    "\n",
    "# 4) Send it to the LLM\n",
    "# from langchain.llms import Ollama\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# llm = Ollama(model=\"llama3\")\n",
    "llm = OllamaLLM(model=\"llama3\")\n",
    "response = llm(final_prompt)\n",
    "\n",
    "print(\"LLM Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 0 section(s). Files written under 'sql_files' with unique ID '814534bfe520410e9ff160e53278fdbf'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "def parse_llm_output_to_sql_files(\n",
    "    llm_output: str,\n",
    "    output_dir: str = \"sql_files\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Parse an LLM response containing up to four markers:\n",
    "      **1) -- Final Query**\n",
    "      **2) -- Insert into queries_log**\n",
    "      **3) -- Create or replace view**\n",
    "      **4) -- Insert into generated_views**\n",
    "    \n",
    "    and write each section that follows into a separate .sql file.\n",
    "\n",
    "    Each file is saved in either 'logging' or 'views' subfolder within 'output_dir',\n",
    "    and is appended with an automatically generated unique ID (UUID).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    llm_output : str\n",
    "        The LLM's generated text containing the four labeled sections.\n",
    "    output_dir : str, optional\n",
    "        The base directory where 'logging' and 'views' subfolders will be created.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    test_response = \\\"\\\"\\\"\n",
    "    **1) -- Final Query**\n",
    "    SELECT * FROM film;\n",
    "\n",
    "    **2) -- Insert into queries_log**\n",
    "    INSERT INTO queries_log(...);\n",
    "\n",
    "    **3) -- Create or replace view**\n",
    "    CREATE OR REPLACE VIEW...;\n",
    "\n",
    "    **4) -- Insert into generated_views**\n",
    "    INSERT INTO generated_views(...);\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    parse_llm_output_to_sql_files(test_response)\n",
    "    \"\"\"\n",
    "\n",
    "    # Automatically generate a unique ID for this parsing run\n",
    "    unique_id = uuid.uuid4().hex  # or str(uuid.uuid4())\n",
    "\n",
    "    # Define the markers (in the order we expect them)\n",
    "    markers = [\n",
    "        \"**1) -- Final Query**\",\n",
    "        \"**2) -- Insert into queries_log**\",\n",
    "        \"**3) -- Create or replace view**\",\n",
    "        \"**4) -- Insert into generated_views**\"\n",
    "    ]\n",
    "\n",
    "    # Map each marker to (base_filename, subfolder)\n",
    "    # We'll append the unique_id to base_filename later when writing\n",
    "    marker_file_map = {\n",
    "        \"**1) -- Final Query**\":           (\"final_query\", \"views\"),\n",
    "        \"**2) -- Insert into queries_log**\": (\"insert_into_queries_log\", \"logging\"),\n",
    "        \"**3) -- Create or replace view**\":  (\"create_or_replace_view\", \"views\"),\n",
    "        \"**4) -- Insert into generated_views**\": (\"insert_into_generated_views\", \"logging\"),\n",
    "    }\n",
    "\n",
    "    # Prepare a dictionary to hold lines for each marker\n",
    "    content_map = {m: [] for m in markers}\n",
    "\n",
    "    # Split the LLM output by lines\n",
    "    lines = llm_output.splitlines()\n",
    "    current_marker = None\n",
    "\n",
    "    # Collect lines under each marker\n",
    "    for line in lines:\n",
    "        line_stripped = line.strip()\n",
    "        if line_stripped in markers:\n",
    "            current_marker = line_stripped\n",
    "        else:\n",
    "            if current_marker:\n",
    "                content_map[current_marker].append(line)\n",
    "\n",
    "    # Ensure the base output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Write out each marker's content to the corresponding file & subfolder\n",
    "    nonempty_count = 0\n",
    "    for marker, (base_name, subfolder) in marker_file_map.items():\n",
    "        block_content = \"\\n\".join(content_map[marker]).strip()\n",
    "        if block_content:\n",
    "            subpath = os.path.join(output_dir, subfolder)\n",
    "            os.makedirs(subpath, exist_ok=True)\n",
    "\n",
    "            filename = f\"{base_name}_{unique_id}.sql\"\n",
    "            file_path = os.path.join(subpath, filename)\n",
    "\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(block_content + \"\\n\")\n",
    "\n",
    "            nonempty_count += 1\n",
    "\n",
    "    print(f\"Parsed {nonempty_count} section(s). Files written under '{output_dir}' with unique ID '{unique_id}'.\")\n",
    "\n",
    "\n",
    "\n",
    "parse_llm_output_to_sql_files(response, output_dir=\"sql_files\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
